## Project Overview

593 semester project: creating a distributed MPC system with data policy enforcement

# How it works

- Split any integer secret into `n` shares.
- Each share (except the last) is a random integer, possibly positive or negative.
- The sum of all shares equals the original secret (last share is calculated based on the previous randomly generated ones).
- Reconstruct the original secret by summing the shares.

# Secret Sharing in Rust

The secret share function splits a secret integer into multiple "shares" so that the sum of all shares is a reconstructed version of the original secret. This is a basic illustration of how secret splitting works for secure multi-party computation (MPC).


## Project Structure
```text
src/
â”‚
â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ client.rs                     # runs client nodes
â”‚   â”œâ”€â”€ server3333.rs                 # server at local host 3333/ server1.txt
â”‚   â””â”€â”€ server3334.rs                 # server at local host 3334/ server2.txt
â”‚
â”œâ”€â”€ .gitignore                        # files to exclude
â”œâ”€â”€ Cargo.lock                        # rust autogenerated file
â”œâ”€â”€ Cargo.toml                        # packages and dependencies
â””â”€â”€ README.md                         # project documentation
```

## Script Details

| Script Name | Role | Description |
| :--- | :--- | :--- |
| **`client.rs`** | **client nodes** | The entry point. Calls all other scripts in the correct order (ETL â†’ Logic â†’ Analysis â†’ Viz). |
| **`server3333/3334.rs`** | **server in distributed system** | Ingests `01_raw` CSVs, cleans headers/dates, and builds the `usaspending.db`. |

Both server files function the same, each has its own local host and txt file location hardcoded internally. 

## How to Run Program


### System Requirements

  * updated version of Rust
  * hello_cargo will handle other dependencies

### Step 2: Set Up Python Environment

We strongly recommend using a virtual environment to manage dependencies.

**Mac/Linux:**

```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows:**

```bash
python -m venv venv
.\venv\Scripts\activate
```

### Step 3: Install Dependencies

All required libraries are listed in the `requirements.txt` file.

```bash
pip install -r requirements.txt
```

### Step 4: Data Preparation

Before running the code, you must place the raw data files in the correct directory so `config.py` can find them.

1.  **Download Data**: Obtain the latest transaction and summary data from USAspending.gov.
2.  **Place Files**: Move the CSV files into the `data/01_raw/` folder.
3.  **Rename/Verify**: Ensure the filenames match what is expected in `scripts/config.py` (or update `config.py` to match your files)
4.  **File Placement**:
  *  **Raw Data** (`data/01_raw/`):
        * `transactions_2020-2025.csv`can be downloaded automatically via the script (Source: USAspending)
        * `Assistance_PrimeAwardSummaries_...csv` (Source: USAspending)
  *  **Mapping Data** (`data/03_primary/`):
        * `city_mapping.json` (Required for normalizing city names, e.g., `{"MARSTONS MILLS": "BARNSTABLE"}`).
        * `population_2020.csv` (Required for calculating financial impact per capita).

### Step 5: Run the Full Pipeline

The entire workflow (ETL â†’ Classification â†’ Analysis â†’ Visualization) is orchestrated by a single master script.
The pipeline is flexible. You can run it in three different modes depending on your needs.

**Mode A: The "Fresh Start" (Download + Full Run):**
Use this mode when you want to fetch the latest data from the API and run the complete analysis. (Warning: Downloading data may take 10-20 minutes depending on network speed).

```bash
python scripts/main_pipeline.py --run_download
```

**Mode B: The "Standard Run" (Local Data)**
Use this mode if you already have the CSV files in data/01_raw/ and just want to rebuild the database and run the analysis.

```bash
python scripts/main_pipeline.py
```

**Mode C: The "Fast Mode" (Skip ETL)**
Use this mode if the database (usaspending.db) is already built and you only want to tweak the classification logic or visualizations. This saves significant time.

```bash
python scripts/main_pipeline.py --skip-etl
```

*Upon completion, check the `outputs/` folder for the final CSV reports and the `images/` folder for visualizations.*


### Configuration

You can adjust the scope of the analysis by modifying `scripts/config.py`:

  * `START_DATE` / `END_DATE`: Range for transaction queries.
  * `START_YEAR` / `END_YEAR`: Fiscal years to analyze.
  * `STATE_FIPS`: Target state (Default is '25' for Massachusetts).


## 6\. ðŸ›  Troubleshooting

Common issues and their solutions:

| Issue | Possible Cause | Solution |
| :--- | :--- | :--- |
| **`FileNotFoundError`** | Raw data is missing or misplaced. | Ensure your CSV files are inside `data/01_raw/` and filenames match `config.py`. |
| **`ModuleNotFoundError`** | Running script from inside `scripts/` folder. | Always run from the **Project Root**: `python scripts/main_pipeline.py`. |
| **`database is locked`** | Database is open in another program. | Close any DB viewers (like DB Browser for SQLite) or running notebooks. |
| **"No data remaining"** | Date filters are too narrow. | Check `START_DATE` and `END_DATE` in `config.py`. Ensure raw CSVs contain data for those years. |


## GIA Disclosure (Generative AI Assistance)

Parts of this document were drafted with the help of an AI assistant and then reviewed/edited by the author. The AI was guided by the repositoryâ€™s source and project context; nonetheless, technical accuracy, compliance, and suitability for your specific environment should be independently verified before use in production.
